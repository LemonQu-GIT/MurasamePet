from fastapi import FastAPI, Request
from datetime import datetime
import uvicorn
import requests
import json
import torch
import platform
import sys
import os
from Murasame.utils import get_config

# æ£€æµ‹å¹³å°å’Œå¼ºåˆ¶è¦æ±‚
IS_MACOS = platform.system() == "Darwin"

if IS_MACOS:
    # åœ¨ macOS ä¸Šå¼ºåˆ¶è¦æ±‚ MLX
    try:
        from mlx_lm.utils import load
        from mlx_lm.generate import generate
        ENGINE = "mlx"
        DEVICE = "mlx"  # MLX ä¼šè‡ªåŠ¨ä½¿ç”¨ Apple Silicon GPU (Metal)
        print("Using MLX engine on macOS (Apple Silicon optimized)")
    except ImportError as e:
        print(f"âŒ CRITICAL ERROR: MLX is required on macOS but not available!")
        print(f"Import error: {e}")
        print()
        print("ğŸ” SOLUTION:")
        print("1. Install MLX: pip install mlx-lm")
        print("2. Or ensure you're using Python with MLX support")
        print()
        print("ğŸš¨ EXITING: macOS requires MLX for optimal performance.")
        exit(1)
else:
    # åœ¨é macOS ç³»ç»Ÿä¸Šä½¿ç”¨ PyTorch
    ENGINE = "torch"
    # æ£€æµ‹è®¾å¤‡ä¼˜å…ˆçº§ï¼šMPS > CUDA > CPU
    if torch.backends.mps.is_available():
        DEVICE = "mps"
    elif torch.cuda.is_available():
        DEVICE = "cuda"
    else:
        DEVICE = "cpu"
    print(f"Using PyTorch engine with device: {DEVICE}")

api = FastAPI()

adapter_path = "./models/Murasame"
max_seq_length = 2048


def load_model_and_tokenizer():
    print(f"Loading model and tokenizer from adapter path: {adapter_path}")
    print(f"Engine: {ENGINE}, Device: {DEVICE}")

    if IS_MACOS:
        # åœ¨ macOS ä¸Šä½¿ç”¨ MLX + LoRA çš„ç»„åˆæ–¹å¼
        print("ğŸ Loading MLX model with LoRA adapter...")

        # åŸºåº•æ¨¡å‹è·¯å¾„ (Qwen3-14B-MLX)
        base_model_path = "./models/Qwen3-14B-MLX"

        # æ£€æŸ¥åŸºåº•æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(base_model_path):
            print(f"âŒ CRITICAL ERROR: Base model not found at {base_model_path}")
            print("Please run download.py first to download the required models.")
            exit(1)

        # æ£€æŸ¥ LoRA é€‚é…å™¨æ˜¯å¦å­˜åœ¨
        if not os.path.exists(adapter_path):
            print(f"âŒ CRITICAL ERROR: LoRA adapter not found at {adapter_path}")
            print("Please run download.py first to download the LoRA adapter.")
            exit(1)

        try:
            # ä½¿ç”¨ MLX åŠ è½½åŸºåº•æ¨¡å‹å’Œ LoRA é€‚é…å™¨
            model, tokenizer = load(base_model_path, adapter_path=adapter_path)
            print("âœ… MLX model with LoRA adapter loaded successfully!")
            print(f"   - Base model: {base_model_path}")
            print(f"   - LoRA adapter: {adapter_path}")

        except Exception as e:
            print(f"âŒ CRITICAL ERROR: Failed to load MLX model with LoRA!")
            print(f"Error details: {e}")
            print()
            print("ğŸ” POSSIBLE CAUSES:")
            print("1. Base model files are corrupted or incomplete")
            print("2. LoRA adapter files are incompatible with MLX")
            print("3. Missing required MLX dependencies")
            print()
            print("ğŸ’¡ SOLUTION:")
            print("1. Re-run download.py to ensure all models are properly downloaded")
            print("2. Check that MLX and mlx-lm are properly installed")
            print("3. Verify the LoRA adapter is compatible with the base model")
            print()
            print("ğŸš¨ EXITING: This application requires working MLX + LoRA setup.")
            exit(1)
    else:
        # åœ¨é macOS ç³»ç»Ÿä¸Šä½¿ç”¨ PyTorch (ä¿æŒåŸæœ‰é€»è¾‘)
        print("Loading PyTorch model with LoRA...")

        try:
            model, tokenizer = load(adapter_path)
            print("LoRA model loaded successfully with PyTorch.")
        except Exception as e:
            print(f"âŒ CRITICAL ERROR: Failed to load LoRA model with PyTorch!")
            print(f"Error details: {e}")
            print()
            print("ğŸ” POSSIBLE CAUSES:")
            print("1. LoRA files are corrupted or incomplete")
            print("2. Missing required PyTorch dependencies")
            print()
            print("ğŸ’¡ SOLUTION:")
            print("Re-run download.py to ensure LoRA files are properly downloaded")
            print()
            print("ğŸš¨ EXITING: This application requires LoRA to function properly.")
            exit(1)

    return model, tokenizer


# è¾…åŠ©å‡½æ•°ï¼šè·å–å½“å‰æ—¶é—´
def get_current_time():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•è¯·æ±‚æ—¥å¿—
def log_request(prompt):
    print(f'[{get_current_time()}] Prompt: {prompt}')


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•å“åº”æ—¥å¿—
def log_response(response):
    print(f'[{get_current_time()}] Final Response: {response}')


# è¾…åŠ©å‡½æ•°ï¼šè§£æè¯·æ±‚
def parse_request(json_post_list):
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    return prompt, history


# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæ ‡å‡†å“åº”
def create_response(response_text, history, status=200):
    time = get_current_time()
    return {
        "response": response_text,
        "history": history,
        "status": status,
        "time": time
    }


# MLX ä¸éœ€è¦æ‰‹åŠ¨åƒåœ¾å›æ”¶


def should_use_openrouter(config):
    """æ£€æµ‹æ˜¯å¦åº”è¯¥ä½¿ç”¨ OpenRouter"""
    api_key = config.get('openrouter_api_key', '')
    return bool(api_key.strip())  # æœ‰éç©ºå€¼å°±ä½¿ç”¨ OpenRouter


def call_openrouter_api(api_key, model, messages, image_url=None):
    """è°ƒç”¨ OpenRouter API"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://murasame-pet.local",
        "X-Title": "MurasamePet"
    }

    # å¤„ç†å›¾åƒè¾“å…¥ - æŒ‰ç…§ OpenRouter å®˜æ–¹æ–‡æ¡£æ ¼å¼
    if image_url:
        # å¦‚æœæœ‰å›¾åƒï¼Œå°†æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ä¿®æ”¹ä¸ºåŒ…å«å›¾åƒ
        for message in reversed(messages):
            if message['role'] == 'user':
                if isinstance(message['content'], str):
                    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå®˜æ–¹æ–‡æ¡£è¦æ±‚çš„æ•°ç»„æ ¼å¼
                    message['content'] = [
                        {"type": "text", "text": message['content']},
                        {"type": "image_url", "image_url": {"url": image_url}}
                    ]
                elif isinstance(message['content'], list):
                    # å¦‚æœå·²ç»æ˜¯æ•°ç»„æ ¼å¼ï¼Œç›´æ¥æ·»åŠ å›¾åƒ
                    message['content'].append({"type": "image_url", "image_url": {"url": image_url}})
                break

    data = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 2048
    }

    # ç¡¬ç¼–ç  OpenRouter åœ°å€
    response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=data)
    response.raise_for_status()
    return response.json()


@api.post("/chat")
async def create_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    log_request(prompt)
    history = history + [{'role': 'user', 'content': prompt}]

    # ä½¿ç”¨ MLX è¿›è¡Œæ¨ç†
    print("Using MLX for inference...")
    text = tokenizer.apply_chat_template(
        history,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )

    # MLX LM æ¨ç†
    response = generate(
        model, tokenizer,
        prompt=text,
        max_tokens=json_post_list.get('max_new_tokens', 2048),
        temp=json_post_list.get('temperature', 0.9),
        top_p=json_post_list.get('top_p', 0.95),
        verbose=False
    )
    reply = response.strip()

    history.append({"role": "assistant", "content": reply})

    log_response(reply)
    return create_response(reply, history)


@api.post("/qwen3")
async def create_qwen3_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    role = json_post_list.get('role', 'user')
    log_request(prompt)
    if prompt != "":
        history = history + [{'role': role, 'content': prompt}]

    # Qwen3-14b å¼ºåˆ¶ä½¿ç”¨æœ¬åœ° LoRA ä»¥ä¿æŒè§’è‰²ç‰¹è‰²
    config = get_config()
    endpoint_url = config['endpoints']['ollama']
    response = requests.post(
        f"{endpoint_url}/api/chat",
        json={"model": "qwen3:14b", "messages": history,
              "stream": False, "options": {"keep_alive": -1}},
    )
    final_response = response.json()['message']['content']

    history = history + [{'role': 'assistant', 'content': final_response}]
    log_response(final_response)
    return create_response(final_response, history)


@api.post("/qwenvl")
async def create_qwenvl_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    log_request(prompt)

    if "image" in json_post_list:
        image_url = json_post_list.get('image')
        history = history + \
            [{'role': 'user', 'content': prompt, 'images': [image_url]}]
    else:
        history = history + [{'role': 'user', 'content': prompt}]

    config = get_config()

    if should_use_openrouter(config):
        # ä½¿ç”¨ OpenRouterï¼Œæ”¯æŒå›¾åƒè¾“å…¥
        api_key = config.get('openrouter_api_key', '')
        image_url = json_post_list.get('image') if "image" in json_post_list else None

        try:
            result = call_openrouter_api(
                api_key,
                "qwen/qwen-2.5-vl-7b-instruct",  # OpenRouter è§†è§‰æ¨¡å‹åç§°
                history,
                image_url=image_url
            )
            final_response = result['choices'][0]['message']['content']
        except Exception as e:
            error_msg = f"OpenRouter API error: {str(e)}"
            log_response(error_msg)
            return create_response(error_msg, history, status=500)
    else:
        # ä½¿ç”¨æœ¬åœ° Ollama API
        endpoint_url = config['endpoints']['ollama']
        response = requests.post(
            f"{endpoint_url}/api/chat",
            json={"model": "qwen2.5vl:7b", "messages": history,
                  "stream": False, "options": {"keep_alive": -1}},
        )
        final_response = response.json()['message']['content']

    history = history + [{'role': 'assistant', 'content': final_response}]
    log_response(final_response)
    return create_response(final_response, history)


if __name__ == '__main__':
    model, tokenizer = load_model_and_tokenizer()
    # MLX ä¸ä½¿ç”¨ TextStreamer
    uvicorn.run(api, host='0.0.0.0', port=28565, workers=1)
