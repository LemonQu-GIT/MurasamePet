# -*- coding: utf-8 -*-
"""
MurasamePet API æœåŠ¡
æä¾›èŠå¤©ã€é—®ç­”å’Œè§†è§‰ç†è§£æ¥å£
"""

from fastapi import FastAPI, Request
from datetime import datetime
import uvicorn
import requests
import json
import torch
import platform
import sys
import os
from Murasame.utils import get_config

# ç¡®ä¿æ ‡å‡†è¾“å‡ºä½¿ç”¨ UTF-8 ç¼–ç ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
if sys.stdout.encoding != 'utf-8':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# æ£€æµ‹å¹³å°å’Œå¼ºåˆ¶è¦æ±‚
IS_MACOS = platform.system() == "Darwin"

if IS_MACOS:
    # åœ¨ macOS ä¸Šå¼ºåˆ¶è¦æ±‚ MLX
    print("ğŸ æ£€æµ‹åˆ° macOS ç³»ç»Ÿï¼Œåˆå§‹åŒ– MLX å¼•æ“...")
    try:
        from mlx_lm.utils import load
        from mlx_lm.generate import generate
        ENGINE = "mlx"
        DEVICE = "mlx"  # MLX ä¼šè‡ªåŠ¨ä½¿ç”¨ Apple Silicon GPU (Metal)
        print("âœ… MLX å¼•æ“åŠ è½½æˆåŠŸ (Apple Silicon GPU åŠ é€Ÿ)")
    except ImportError as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šmacOS ç³»ç»Ÿéœ€è¦ MLX ä½†æœªæ‰¾åˆ°è¯¥åº“ï¼")
        print(f"å¯¼å…¥é”™è¯¯è¯¦æƒ…: {e}")
        print()
        print("ğŸ” è§£å†³æ–¹æ¡ˆï¼š")
        print("1. å®‰è£… MLX: pip install mlx-lm")
        print("2. æˆ–ç¡®ä¿æ‚¨ä½¿ç”¨çš„ Python ç¯å¢ƒæ”¯æŒ MLX")
        print()
        print("ğŸš¨ ç¨‹åºé€€å‡ºï¼šmacOS ç³»ç»Ÿå¿…é¡»ä½¿ç”¨ MLX ä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        exit(1)
else:
    # åœ¨é macOS ç³»ç»Ÿä¸Šä½¿ç”¨ PyTorch
    print("ğŸ–¥ï¸ æ£€æµ‹åˆ°é macOS ç³»ç»Ÿï¼Œåˆå§‹åŒ– PyTorch å¼•æ“...")
    ENGINE = "torch"
    # æ£€æµ‹è®¾å¤‡ä¼˜å…ˆçº§ï¼šMPS > CUDA > CPU
    if torch.backends.mps.is_available():
        DEVICE = "mps"
        print("âœ… PyTorch å¼•æ“åŠ è½½æˆåŠŸ (ä½¿ç”¨ MPS åŠ é€Ÿ)")
    elif torch.cuda.is_available():
        DEVICE = "cuda"
        print("âœ… PyTorch å¼•æ“åŠ è½½æˆåŠŸ (ä½¿ç”¨ CUDA åŠ é€Ÿ)")
    else:
        DEVICE = "cpu"
        print("âš ï¸ PyTorch å¼•æ“åŠ è½½æˆåŠŸ (ä½¿ç”¨ CPUï¼Œæ€§èƒ½å¯èƒ½è¾ƒæ…¢)")

api = FastAPI()

adapter_path = "./models/Murasame"
max_seq_length = 2048


def load_model_and_tokenizer():
    print(f"ğŸ“‚ æ¨¡å‹åŠ è½½è·¯å¾„: {adapter_path}")
    print(f"âš™ï¸ æ¨ç†å¼•æ“: {ENGINE} | è®¡ç®—è®¾å¤‡: {DEVICE}")

    if IS_MACOS:
        # åœ¨ macOS ä¸Šä½¿ç”¨å·²åˆå¹¶çš„ MLX æ¨¡å‹
        print("ğŸ æ­£åœ¨åŠ è½½åˆå¹¶åçš„ MLX æ¨¡å‹ (Qwen3-14B-Murasame-Chat-MLX-Int4)...")

        # æ£€æŸ¥åˆå¹¶åçš„æ¨¡å‹æ˜¯å¦å­˜åœ¨
        if not os.path.exists(adapter_path):
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæœªæ‰¾åˆ°åˆå¹¶æ¨¡å‹ {adapter_path}")
            print("ğŸ’¡ è¯·å…ˆè¿è¡Œ download.py ä¸‹è½½åˆå¹¶æ¨¡å‹")
            exit(1)

        try:
            print("ğŸ”„ æ­£åœ¨ä»ç£ç›˜è¯»å–æ¨¡å‹æ–‡ä»¶...")
            # ç›´æ¥åŠ è½½åˆå¹¶åçš„å®Œæ•´æ¨¡å‹ï¼ˆä¸éœ€è¦å•ç‹¬çš„ base_model å’Œ adapterï¼‰
            model, tokenizer = load(adapter_path)
            print("âœ… åˆå¹¶ MLX æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   ğŸ“ æ¨¡å‹è·¯å¾„: {adapter_path}")
            print(f"   ğŸ·ï¸ æ¨¡å‹ç±»å‹: Qwen3-14B + Murasame LoRA (å·²åˆå¹¶, Int4 é‡åŒ–)")
            print(f"   ğŸš€ å·²å¯ç”¨ Apple Silicon GPU åŠ é€Ÿ")

        except Exception as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åŠ è½½åˆå¹¶ MLX æ¨¡å‹ï¼")
            print(f"é”™è¯¯è¯¦æƒ…: {e}")
            print()
            print("ğŸ” å¯èƒ½çš„åŸå› ï¼š")
            print("1. æ¨¡å‹æ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´")
            print("2. ä¸‹è½½çš„æ¨¡å‹ç‰ˆæœ¬ä¸ MLX ä¸å…¼å®¹")
            print("3. ç¼ºå°‘å¿…éœ€çš„ MLX ä¾èµ–")
            print()
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            print("1. é‡æ–°è¿è¡Œ download.py ç¡®ä¿åˆå¹¶æ¨¡å‹æ­£ç¡®ä¸‹è½½")
            print("2. æ£€æŸ¥ MLX å’Œ mlx-lm æ˜¯å¦æ­£ç¡®å®‰è£… (pip install mlx-lm)")
            print("3. éªŒè¯ ./models/Murasame ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶")
            print()
            print("ğŸš¨ ç¨‹åºé€€å‡ºï¼šåº”ç”¨éœ€è¦åˆå¹¶ MLX æ¨¡å‹æ‰èƒ½è¿è¡Œ")
            exit(1)
    else:
        # åœ¨é macOS ç³»ç»Ÿä¸Šä½¿ç”¨ PyTorch (ä¿æŒåŸæœ‰é€»è¾‘)
        print("ğŸ”§ æ­£åœ¨åŠ è½½ PyTorch LoRA æ¨¡å‹...")

        try:
            print("ğŸ”„ æ­£åœ¨ä»ç£ç›˜è¯»å–æ¨¡å‹æ–‡ä»¶...")
            model, tokenizer = load(adapter_path)
            print("âœ… LoRA æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   ğŸ“ æ¨¡å‹è·¯å¾„: {adapter_path}")
            print(f"   ğŸ·ï¸ æ¨¡å‹ç±»å‹: PyTorch LoRA")
        except Exception as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼šæ— æ³•åŠ è½½ PyTorch LoRA æ¨¡å‹ï¼")
            print(f"é”™è¯¯è¯¦æƒ…: {e}")
            print()
            print("ğŸ” å¯èƒ½çš„åŸå› ï¼š")
            print("1. LoRA æ–‡ä»¶æŸåæˆ–ä¸å®Œæ•´")
            print("2. ç¼ºå°‘å¿…éœ€çš„ PyTorch ä¾èµ–")
            print()
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            print("é‡æ–°è¿è¡Œ download.py ç¡®ä¿ LoRA æ–‡ä»¶æ­£ç¡®ä¸‹è½½")
            print()
            print("ğŸš¨ ç¨‹åºé€€å‡ºï¼šåº”ç”¨éœ€è¦ LoRA æ¨¡å‹æ‰èƒ½è¿è¡Œ")
            exit(1)

    return model, tokenizer


# è¾…åŠ©å‡½æ•°ï¼šè·å–å½“å‰æ—¶é—´
def get_current_time():
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•è¯·æ±‚æ—¥å¿—
def log_request(prompt):
    print(f'ğŸ“¥ [{get_current_time()}] æ”¶åˆ°ç”¨æˆ·è¯·æ±‚: {prompt}')


# è¾…åŠ©å‡½æ•°ï¼šè®°å½•å“åº”æ—¥å¿—
def log_response(response):
    print(f'ğŸ“¤ [{get_current_time()}] ç”Ÿæˆæœ€ç»ˆå›å¤: {response}')


# è¾…åŠ©å‡½æ•°ï¼šè§£æè¯·æ±‚
def parse_request(json_post_list):
    prompt = json_post_list.get('prompt')
    history = json_post_list.get('history')
    return prompt, history


# è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºæ ‡å‡†å“åº”
def create_response(response_text, history, status=200):
    time = get_current_time()
    return {
        "response": response_text,
        "history": history,
        "status": status,
        "time": time
    }


# MLX ä¸éœ€è¦æ‰‹åŠ¨åƒåœ¾å›æ”¶


def should_use_openrouter(config):
    """æ£€æµ‹æ˜¯å¦åº”è¯¥ä½¿ç”¨ OpenRouter"""
    api_key = config.get('openrouter_api_key', '')
    return bool(api_key.strip())  # æœ‰éç©ºå€¼å°±ä½¿ç”¨ OpenRouter


def call_openrouter_api(api_key, model, messages, image_url=None, max_tokens=2048):
    """è°ƒç”¨ OpenRouter API"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://murasame-pet.local",
        "X-Title": "MurasamePet"
    }

    # å¤„ç†å›¾åƒè¾“å…¥ - æŒ‰ç…§ OpenRouter å®˜æ–¹æ–‡æ¡£æ ¼å¼
    if image_url:
        # å¦‚æœæœ‰å›¾åƒï¼Œå°†æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ä¿®æ”¹ä¸ºåŒ…å«å›¾åƒ
        for message in reversed(messages):
            if message['role'] == 'user':
                if isinstance(message['content'], str):
                    # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå®˜æ–¹æ–‡æ¡£è¦æ±‚çš„æ•°ç»„æ ¼å¼
                    message['content'] = [
                        {"type": "text", "text": message['content']},
                        {"type": "image_url", "image_url": {"url": image_url}}
                    ]
                elif isinstance(message['content'], list):
                    # å¦‚æœå·²ç»æ˜¯æ•°ç»„æ ¼å¼ï¼Œç›´æ¥æ·»åŠ å›¾åƒ
                    message['content'].append({"type": "image_url", "image_url": {"url": image_url}})
                break

    data = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": max_tokens
    }

    # ç¡¬ç¼–ç  OpenRouter åœ°å€
    response = requests.post("https://openrouter.ai/api/v1/chat/completions", headers=headers, json=data)
    response.raise_for_status()
    return response.json()


@api.post("/chat")
async def create_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    log_request(prompt)
    history = history + [{'role': 'user', 'content': prompt}]

    # ä½¿ç”¨ MLX è¿›è¡Œæ¨ç†
    print("ğŸ’¬ ä½¿ç”¨ MLX å¼•æ“è¿›è¡Œæ¨ç†...")
    print(f"ğŸ“Š æœ€å¤§ç”Ÿæˆé•¿åº¦: {json_post_list.get('max_new_tokens', 2048)} tokens")
    
    text = tokenizer.apply_chat_template(
        history,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=False,
    )
    print("âœ… èŠå¤©æ¨¡æ¿åº”ç”¨å®Œæˆ")

    # MLX LM æ¨ç†
    print("ğŸ¤– æ­£åœ¨ç”Ÿæˆå›å¤...")
    response = generate(
        model, tokenizer,
        prompt=text,
        max_tokens=json_post_list.get('max_new_tokens', 2048),
        verbose=False
    )
    reply = response.strip()
    print(f"âœ… å›å¤ç”Ÿæˆå®Œæˆ (é•¿åº¦: {len(reply)} å­—ç¬¦)")

    history.append({"role": "assistant", "content": reply})

    log_response(reply)
    return create_response(reply, history)


@api.post("/qwen3")
async def create_qwen3_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    role = json_post_list.get('role', 'user')
    log_request(prompt)
    if prompt != "":
        history = history + [{'role': role, 'content': prompt}]

    config = get_config()

    if should_use_openrouter(config):
        # ä¼˜å…ˆä½¿ç”¨ OpenRouter çš„ qwen3-235b æ¨¡å‹
        print("ğŸŒ ä½¿ç”¨ OpenRouter API (qwen3-235b-a22b æ¨¡å‹)...")
        api_key = config.get('openrouter_api_key', '')
        try:
            print("ğŸ”„ æ­£åœ¨è°ƒç”¨ OpenRouter API...")
            result = call_openrouter_api(
                api_key,
                "qwen/qwen3-235b-a22b",  # ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
                history,
                max_tokens=4096  # è¾…åŠ©åŠŸèƒ½å¯èƒ½éœ€è¦æ›´å¤š tokens
            )
            final_response = result['choices'][0]['message']['content']
            print("âœ… OpenRouter API è°ƒç”¨æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ OpenRouter API è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ° Ollama: {e}")
            # å›é€€åˆ° Ollama
            print("ğŸ”„ æ­£åœ¨åˆ‡æ¢åˆ°æœ¬åœ° Ollama æœåŠ¡...")
            endpoint_url = config['endpoints']['ollama']
            response = None
            try:
                print(f"ğŸ“¡ æ­£åœ¨è°ƒç”¨ Ollama API ({endpoint_url})...")
                response = requests.post(
                    f"{endpoint_url}/api/chat",
                    json={"model": "qwen3:14b", "messages": history,
                          "stream": False, "options": {"keep_alive": -1}},
                )
                print(f"ğŸ“Š Ollama å“åº”çŠ¶æ€: {response.status_code}")
                print(f"ğŸ“‹ Ollama å“åº”å¤´: {response.headers}")
                print(f"ğŸ“„ Ollama å“åº”å†…å®¹ (å‰500å­—ç¬¦): {response.text[:500]}")
                final_response = response.json()['message']['content']
                print("âœ… Ollama API è°ƒç”¨æˆåŠŸ")
            except requests.exceptions.JSONDecodeError as e:
                print(f"âŒ Ollama JSON è§£æé”™è¯¯: {e}")
                if response:
                    print(f"å“åº”çŠ¶æ€: {response.status_code}")
                    print(f"å“åº”å†…å®¹: {response.text}")
                    raise Exception(f"Ollama API è¿”å›äº†æ— æ•ˆçš„ JSONã€‚çŠ¶æ€: {response.status_code}, å“åº”: {response.text[:500]}")
                else:
                    raise Exception("Ollama API è¿”å›äº†æ— æ•ˆçš„ JSONã€‚æœªæ”¶åˆ°å“åº”ã€‚")
            except Exception as e:
                print(f"âŒ è°ƒç”¨ Ollama API æ—¶å‡ºé”™: {e}")
                raise
    else:
        # ä½¿ç”¨ Ollama
        print("ğŸ  ä½¿ç”¨æœ¬åœ° Ollama API (qwen3:14b æ¨¡å‹)...")
        endpoint_url = config['endpoints']['ollama']
        response = None
        try:
            print(f"ğŸ“¡ æ­£åœ¨è°ƒç”¨ Ollama API ({endpoint_url})...")
            response = requests.post(
                f"{endpoint_url}/api/chat",
                json={"model": "qwen3:14b", "messages": history,
                      "stream": False, "options": {"keep_alive": -1}},
            )
            print(f"ğŸ“Š Ollama å“åº”çŠ¶æ€: {response.status_code}")
            print(f"ğŸ“‹ Ollama å“åº”å¤´: {response.headers}")
            print(f"ğŸ“„ Ollama å“åº”å†…å®¹ (å‰500å­—ç¬¦): {response.text[:500]}")
            final_response = response.json()['message']['content']
            print("âœ… Ollama API è°ƒç”¨æˆåŠŸ")
        except requests.exceptions.JSONDecodeError as e:
            print(f"âŒ Ollama JSON è§£æé”™è¯¯: {e}")
            if response:
                print(f"å“åº”çŠ¶æ€: {response.status_code}")
                print(f"å“åº”å†…å®¹: {response.text}")
                raise Exception(f"Ollama API è¿”å›äº†æ— æ•ˆçš„ JSONã€‚çŠ¶æ€: {response.status_code}, å“åº”: {response.text[:500]}")
            else:
                raise Exception("Ollama API è¿”å›äº†æ— æ•ˆçš„ JSONã€‚æœªæ”¶åˆ°å“åº”ã€‚")
        except Exception as e:
            print(f"âŒ è°ƒç”¨ Ollama API æ—¶å‡ºé”™: {e}")
            raise

    history = history + [{'role': 'assistant', 'content': final_response}]
    log_response(final_response)
    return create_response(final_response, history)


@api.post("/qwenvl")
async def create_qwenvl_chat(request: Request):
    json_post_list = await request.json()
    prompt, history = parse_request(json_post_list)
    log_request(prompt)

    if "image" in json_post_list:
        image_url = json_post_list.get('image')
        print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒè¾“å…¥: {image_url[:100]}...")
        history = history + \
            [{'role': 'user', 'content': prompt, 'images': [image_url]}]
    else:
        print("ğŸ“ çº¯æ–‡æœ¬æ¨¡å¼ï¼ˆæ— å›¾åƒè¾“å…¥ï¼‰")
        history = history + [{'role': 'user', 'content': prompt}]

    config = get_config()

    if should_use_openrouter(config):
        # ä½¿ç”¨ OpenRouterï¼Œæ”¯æŒå›¾åƒè¾“å…¥
        print("ğŸŒ ä½¿ç”¨ OpenRouter API (qwen-2.5-vl-7b-instruct è§†è§‰æ¨¡å‹)...")
        api_key = config.get('openrouter_api_key', '')
        image_url = json_post_list.get('image') if "image" in json_post_list else None

        try:
            print("ğŸ”„ æ­£åœ¨è°ƒç”¨ OpenRouter è§†è§‰ API...")
            result = call_openrouter_api(
                api_key,
                "qwen/qwen-2.5-vl-7b-instruct",  # OpenRouter è§†è§‰æ¨¡å‹åç§°
                history,
                image_url=image_url
            )
            final_response = result['choices'][0]['message']['content']
            print("âœ… OpenRouter è§†è§‰ API è°ƒç”¨æˆåŠŸ")
        except Exception as e:
            error_msg = f"OpenRouter API é”™è¯¯: {str(e)}"
            print(f"âŒ {error_msg}")
            log_response(error_msg)
            return create_response(error_msg, history, status=500)
    else:
        # ä½¿ç”¨æœ¬åœ° Ollama API
        print("ğŸ  ä½¿ç”¨æœ¬åœ° Ollama API (qwen2.5vl:7b è§†è§‰æ¨¡å‹)...")
        endpoint_url = config['endpoints']['ollama']
        print(f"ğŸ“¡ æ­£åœ¨è°ƒç”¨ Ollama è§†è§‰ API ({endpoint_url})...")
        response = requests.post(
            f"{endpoint_url}/api/chat",
            json={"model": "qwen2.5vl:7b", "messages": history,
                  "stream": False, "options": {"keep_alive": -1}},
        )
        final_response = response.json()['message']['content']
        print("âœ… Ollama è§†è§‰ API è°ƒç”¨æˆåŠŸ")

    history = history + [{'role': 'assistant', 'content': final_response}]
    log_response(final_response)
    return create_response(final_response, history)


if __name__ == '__main__':
    print("=" * 60)
    print("ğŸš€ MurasamePet API æœåŠ¡å¯åŠ¨ä¸­...")
    print("=" * 60)
    
    model, tokenizer = load_model_and_tokenizer()
    
    print("=" * 60)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¯åŠ¨ FastAPI æœåŠ¡å™¨...")
    print(f"ğŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:28565")
    print(f"ğŸ“¡ å¯ç”¨ç«¯ç‚¹:")
    print(f"   - POST /chat    (ä¸»å¯¹è¯æ¥å£ - Murasame)")
    print(f"   - POST /qwen3   (é€šç”¨é—®ç­”æ¥å£ - Qwen3)")
    print(f"   - POST /qwenvl  (è§†è§‰ç†è§£æ¥å£ - Qwen-VL)")
    print("=" * 60)
    
    uvicorn.run(api, host='0.0.0.0', port=28565, workers=1)
